{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excess Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, impute_strategy=\"mean\", encode_type=\"onehot\"):\n",
    "        self.impute_strategy = impute_strategy\n",
    "        self.encode_type = encode_type\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "        self.imputers = {\n",
    "            \"host_response_rate\": SimpleImputer(strategy=\"constant\", fill_value=0),\n",
    "            \"review_scores_rating\": SimpleImputer(strategy=\"median\"),\n",
    "            \"first_review\": SimpleImputer(strategy=\"constant\", fill_value=\"2000-01-01\"),\n",
    "            \"last_review\": SimpleImputer(strategy=\"constant\", fill_value=\"2000-01-01\"),\n",
    "            \"thumbnail_url\": SimpleImputer(strategy=\"constant\", fill_value=\"missing_thumbnail\"),\n",
    "            \"neighbourhood\": SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "            \"zipcode\": SimpleImputer(strategy=\"constant\", fill_value=\"00000\"),\n",
    "            \"bathrooms\": SimpleImputer(strategy=\"median\"),\n",
    "            \"bedrooms\": SimpleImputer(strategy=\"median\"),\n",
    "            \"beds\": SimpleImputer(strategy=\"median\"),\n",
    "            \"host_has_profile_pic\": SimpleImputer(strategy=\"constant\", fill_value=\"f\"),\n",
    "            \"host_identity_verified\": SimpleImputer(strategy=\"constant\", fill_value=\"f\"),\n",
    "            \"host_since\": SimpleImputer(strategy=\"constant\", fill_value=\"2000-01-01\"),\n",
    "        }\n",
    "\n",
    "        self.encoders = {\n",
    "            \"mapping\": {\n",
    "                \"room_type\": {\n",
    "                    \"Private room\": 0,\n",
    "                    \"Entire home/apt\": 1,\n",
    "                    \"Shared room\": 2\n",
    "                },\n",
    "                \"bed_type\": {\n",
    "                    \"Real Bed\": 5,\n",
    "                    \"Futon\": 4,\n",
    "                    \"Pull-out Sofa\": 3,\n",
    "                    \"Airbed\": 2,\n",
    "                    \"Couch\": 1,\n",
    "                },\n",
    "                \"cancellation_policy_map_s\": {\n",
    "                    'flexible': 0.0,\n",
    "                    'moderate': 0.33,\n",
    "                    'strict': 0.66,\n",
    "                    'super_strict_30': 0.83,\n",
    "                    'super_strict_60': 1.0\n",
    "                },\n",
    "                \"cancellation_policy_map_f\": {\n",
    "                    'flexible': 1.0,\n",
    "                    'moderate': 0.66,\n",
    "                    'strict': 0.33,\n",
    "                    'super_strict_30': 0.16,\n",
    "                    'super_strict_60': 0.0\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        self.bool_cols = [\"host_has_profile_pic\", \"host_identity_verified\", \"instant_bookable\"]\n",
    "        self.map_cols = [\"bed_type\", \"room_type\"]\n",
    "\n",
    "        self.city_sentiment = {\n",
    "            \"NYC\": 0.75, \"LA\": 0.78, \"SF\": 0.85, \"DC\": 0.73, \"Chicago\": 0.60, \"Boston\": 0.90\n",
    "        }\n",
    "\n",
    "        self.city_expense_worth = {\n",
    "            \"NYC\": 0.55, \"LA\": 0.58, \"SF\": 0.50, \"DC\": 0.78, \"Chicago\": 0.72, \"Boston\": 0.75\n",
    "        }\n",
    "        self.city_centers = {\n",
    "            'NYC': (40.7549, -73.984),\n",
    "            'LA': (34.0557, -118.2488),\n",
    "            'SF': (37.7876, -122.4066),\n",
    "            'DC': (38.9037, -77.0363),\n",
    "            'Chicago': (41.8757, -87.6243),\n",
    "            'Boston': (42.3555, -71.0565)\n",
    "        }\n",
    "\n",
    "    def sentiment_score(self, X):\n",
    "        def compute_sentiment(text):\n",
    "            if not isinstance(text, str) or text.strip() == \"\":\n",
    "                return 0\n",
    "            return self.analyzer.polarity_scores(text)['compound']\n",
    "        X['sentiment'] = X['description'].apply(compute_sentiment)\n",
    "        return X\n",
    "\n",
    "    def objectivity_score(self, X):\n",
    "        def compute_obj(text):\n",
    "            if not isinstance(text, str): return 0\n",
    "            return 1 - TextBlob(text).sentiment.subjectivity\n",
    "        X['objectivity'] = X['description'].apply(compute_obj)\n",
    "        return X\n",
    "\n",
    "    def combine_sentiment_subjectivity(self, sentiment, objectivity, sentiment_weight=0.7, objectivity_weight=0.3):\n",
    "        sentiment_norm = (sentiment + 1) / 2\n",
    "        return sentiment_weight * sentiment_norm + objectivity_weight * objectivity\n",
    "\n",
    "    def day_since(self, start, end):\n",
    "        start = pd.to_datetime(start, errors=\"coerce\")\n",
    "        end = pd.to_datetime(end, errors=\"coerce\")\n",
    "        return (end - start).days if pd.notnull(start) and pd.notnull(end) else -1\n",
    "    \n",
    "    def _compute_amenity_score(self, X, top_k=30):\n",
    "        # Clean and standardize\n",
    "        pet_map = [\n",
    "        \"Pets live on this property\", \"Pets allowed\", \"Dog(s)\", \"Cat(s)\", \"Other pet(s)\"\n",
    "        ]\n",
    "        amenities_map = {\n",
    "            \"Wireless Internet\": \"Internet\",\n",
    "            \"Dryer\": \"Dryer/Washer\",\n",
    "            \"Washer\": \"Dryer/Washer\",\n",
    "            \"Dishwasher\": \"Dryer/Washer\",\n",
    "            \"Central Heating\": \"Heating\",\n",
    "            **{p: \"Pet-Friendly\" for p in pet_map}\n",
    "        }\n",
    "\n",
    "        def map_amenities(amenity_list):\n",
    "            return [amenities_map.get(a.strip().strip('\"'), a.strip().strip('\"')) for a in amenity_list]\n",
    "\n",
    "        X = X.copy()\n",
    "        X['standard_amenities'] = X['split_amenities'].apply(map_amenities)\n",
    "        X['amenities_str'] = X['standard_amenities'].apply(lambda x: ','.join(x))\n",
    "\n",
    "        vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(','), lowercase=False)\n",
    "        tfidf = vectorizer.fit_transform(X['amenities_str'])\n",
    "        tfidf_df = pd.DataFrame(tfidf.toarray(), columns=vectorizer.get_feature_names_out(), index=X.index)\n",
    "\n",
    "        top_amenities = tfidf_df.mean().sort_values(ascending=False).head(top_k).index\n",
    "        tfidf_top_df = tfidf_df[top_amenities]\n",
    "\n",
    "        X['amenity_score'] = tfidf_top_df.sum(axis=1)\n",
    "        scaler = MinMaxScaler()\n",
    "        X['amenity_score_normalized'] = scaler.fit_transform(X[['amenity_score']])\n",
    "\n",
    "        return pd.concat([X, tfidf_top_df], axis=1)\n",
    "    \n",
    "    def haversine(self, lat1, lon1, lat2, lon2):\n",
    "        R = 6371  # km\n",
    "        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "        return R * 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    \n",
    "    def add_distance_to_city_center(self, X, city_col='city', lat_col='latitude', lon_col='longitude'):\n",
    "        def compute_distance(row):\n",
    "            city = row[city_col]\n",
    "            if city in self.city_centers:\n",
    "                center_lat, center_lon = self.city_centers[city]\n",
    "                return self.haversine(row[lat_col], row[lon_col], center_lat, center_lon)\n",
    "            else:\n",
    "                return np.nan  # If city isn't found\n",
    "        X = X.copy()\n",
    "        X['distance_to_city_center'] = X.apply(compute_distance, axis=1)\n",
    "        return X\n",
    "\n",
    "    def fit(self, X):\n",
    "        X = X.copy()\n",
    "        for col, imputer in self.imputers.items():\n",
    "            imputer.fit(X[[col]])\n",
    "            X[col] = imputer.transform(X[[col]]).ravel()\n",
    "\n",
    "        self.bool_encoders = {}\n",
    "        for col in self.bool_cols:\n",
    "            enc = OrdinalEncoder(categories=[[\"f\", \"t\"]], dtype=int, handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "            enc.fit(X[[col]])\n",
    "            self.bool_encoders[col] = enc\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col, imputer in self.imputers.items():\n",
    "            X[col] = imputer.transform(X[[col]]).ravel()\n",
    "\n",
    "        X['first_review'] = pd.to_datetime(X['first_review'], errors=\"coerce\")\n",
    "        X['last_review'] = pd.to_datetime(X['last_review'], errors=\"coerce\")\n",
    "        X['missing_review_dates'] = X['first_review'].isna() | X['last_review'].isna()\n",
    "\n",
    "        mask = X['first_review'].notna() & X['last_review'].notna()\n",
    "        X['review_gap_days'] = np.nan\n",
    "        X.loc[mask, 'review_gap_days'] = (X.loc[mask, 'last_review'] - X.loc[mask, 'first_review']).dt.days\n",
    "        X['review_gap_days'] = X['review_gap_days'].fillna(-1)\n",
    "\n",
    "        X['host_response_rate'] = X['host_response_rate'].astype(str).str.rstrip('%').astype(float)\n",
    "        X['cleaning_fee'] = X['cleaning_fee'].astype(int)\n",
    "\n",
    "        for col in self.bool_cols:\n",
    "            if col not in X.columns:\n",
    "                X[col] = \"f\"\n",
    "            X[col] = self.bool_encoders[col].transform(X[[col]]).ravel()\n",
    "\n",
    "        for col, mapping in self.encoders[\"mapping\"].items():\n",
    "            if col in X.columns:\n",
    "                X[col] = X[col].map(mapping)\n",
    "\n",
    "        if 'latitude' in X.columns:\n",
    "            X['lat_bin'] = pd.cut(X['latitude'], bins=10)\n",
    "        if 'longitude' in X.columns:\n",
    "            X['long_bin'] = pd.cut(X['longitude'], bins=10)\n",
    "\n",
    "        if 'amenities' in X.columns:\n",
    "            X['split_amenities'] = X['amenities'].fillna('').apply(lambda x: str(x).strip(\"{}\").split(','))\n",
    "            X['n_amenities'] = X['amenities'].fillna('').apply(lambda x: len(str(x).strip(\"{}\").split(',')))\n",
    "\n",
    "        X = self.sentiment_score(X)\n",
    "        X = self.objectivity_score(X)\n",
    "        X['description_score'] = X.apply(\n",
    "            lambda row: self.combine_sentiment_subjectivity(row['sentiment'], row['objectivity']), axis=1)\n",
    "\n",
    "        X['luxury_policy_flag'] = X['cancellation_policy'].isin(['super_strict_30', 'super_strict_60']).astype(int)\n",
    "\n",
    "        X['city_value_score'] = X['city'].map(self.city_sentiment)\n",
    "        X['city_expense_score'] = X['city'].map(self.city_expense_worth)\n",
    "\n",
    "        X['days_between_reviews'] = X.apply(\n",
    "            lambda row: self.day_since(row['first_review'], row['last_review']), axis=1)\n",
    "        \n",
    "        most_recent_possible = pd.to_datetime(\"2017-10-04\")\n",
    "\n",
    "        X['host_tenure'] = X.apply(\n",
    "            lambda row: self.day_since(row['host_since'], row['last_review'])\n",
    "            if pd.notnull(row['last_review']) and \n",
    "                pd.to_datetime(row['last_review']) >= pd.to_datetime(row['host_since'])\n",
    "            else self.day_since(row['host_since'], most_recent_possible),\n",
    "            axis=1\n",
    "        )\n",
    "        X = self.add_distance_to_city_center(X)\n",
    "\n",
    "        if 'split_amenities' in X.columns:\n",
    "            X = self._compute_amenity_score(X)\n",
    "        drop_cols = ['id', 'name', 'description', 'thumbnail_url', 'neighbourhood', 'amenities', 'first_review', 'host_since', 'last_review', 'zipcode', 'missing_review_dates']\n",
    "        X = X.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "\n",
    "class ClusterFit(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, cluster_features, n_clusters=4, model_class=GradientBoostingRegressor, model_params=None):\n",
    "        self.cluster_features = cluster_features\n",
    "        self.n_clusters = n_clusters\n",
    "        self.model_class = model_class\n",
    "        self.model_params = model_params or {}\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=42)\n",
    "        self.models = {}  # cluster_id -> trained model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Scale cluster features and fit KMeans\n",
    "        X_cluster = self.scaler.fit_transform(X[self.cluster_features])\n",
    "        clusters = self.kmeans.fit_predict(X_cluster)\n",
    "        self.cluster_assignments_ = clusters\n",
    "        self.unique_clusters_ = np.unique(clusters)\n",
    "\n",
    "        self.X_test_all = []\n",
    "        self.y_test_all = []\n",
    "\n",
    "        # Fit separate model per cluster\n",
    "        for cluster in self.unique_clusters_:\n",
    "            idx = clusters == cluster\n",
    "            X_cluster_data = X[idx].drop(columns=['log_price', 'cluster_label'], errors='ignore')\n",
    "            y_cluster = y[idx]\n",
    "\n",
    "            study = optuna.create_study(direction=\"minimize\")\n",
    "            study.optimize(lambda trial: self._objective(trial, X_cluster_data, y_cluster), n_trials=15)\n",
    "            print(\"Best RMSE:\", study.best_value)\n",
    "            print(\"Best hyperparameters:\", study.best_params)\n",
    "            best_params = study.best_params\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_cluster_data, y_cluster, test_size=0.2, random_state=42\n",
    "            )\n",
    "            \n",
    "            model = self.model_class(**self.model_params)\n",
    "            model.fit(X_train, y_train)\n",
    "            self.models[cluster] = model\n",
    "\n",
    "            self.X_test_all.append(X_test)\n",
    "            self.y_test_all.append(y_test)\n",
    "\n",
    "        return self\n",
    "    def _objective(self, trial, X_cluster_data, y_cluster):\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_cluster_data, y_cluster, test_size=0.2, random_state=42)\n",
    "\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
    "            'random_state': 42\n",
    "        }\n",
    "\n",
    "        model = RandomForestRegressor(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        return rmse\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_cluster = self.scaler.transform(X[self.cluster_features])\n",
    "        clusters = self.kmeans.predict(X_cluster)\n",
    "\n",
    "        preds = np.zeros(len(X))\n",
    "        for cluster in self.unique_clusters_:\n",
    "            idx = clusters == cluster\n",
    "            if np.sum(idx) == 0:\n",
    "                continue\n",
    "            X_cluster_data = X.iloc[idx].drop(columns=['log_price', 'cluster_label'], errors='ignore')\n",
    "            preds[idx] = self.models[cluster].predict(X_cluster_data)\n",
    "        return preds\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return np.sqrt(mean_squared_error(y, y_pred))  # negative RMSE for use in cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_id in [0, 1, 2]:\n",
    "    mask = cluster_labels == cluster_id\n",
    "    y_cluster = y[mask]\n",
    "    preds_cluster = preds[mask]\n",
    "    \n",
    "    se = ((y_cluster - preds_cluster) ** 2).sum()\n",
    "    total_squared_error += se\n",
    "    total_samples += len(y_cluster)\n",
    "\n",
    "overall_rmse_from_clusters = np.sqrt(total_squared_error / total_samples)\n",
    "print(f\"Verified Overall RMSE from cluster breakdown: {overall_rmse_from_clusters:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPerCluster:\n",
    "    def __init__(self, features, model_types, n_trials=30, timeout=300):\n",
    "        self.features = features\n",
    "        self.n_clusters = 3\n",
    "        self.model_types = model_types\n",
    "        self.n_trials = n_trials\n",
    "        self.timeout = timeout\n",
    "\n",
    "        self.scalar = StandardScaler()\n",
    "        self.cluster_models = {}\n",
    "        self.cluster_studies = {}\n",
    "        self.cluster_rmses = {}\n",
    "        self.cluster_model_types = {}\n",
    "\n",
    "    def fit_cluster_models(self, X, y, cluster_labels):\n",
    "        for cluster_id in range(self.n_clusters):\n",
    "            print(f\"\\n>>> Tuning cluster {cluster_id}\")\n",
    "\n",
    "            mask = cluster_labels == cluster_id\n",
    "            X_cluster = X[mask].copy()\n",
    "            y_cluster = y[mask].copy()\n",
    "\n",
    "            best_rmse = float('inf')\n",
    "            best_model = None\n",
    "            best_study = None\n",
    "            best_model_type = None\n",
    "\n",
    "            optuna_params = {}\n",
    "\n",
    "            for name, (model_class, param_space) in self.model_types.items():\n",
    "                if name == \"Stacking\":\n",
    "                    continue\n",
    "\n",
    "                def theobjective(trial):\n",
    "                    params = param_space(trial)\n",
    "                    X_train, X_val, y_train, y_val = train_test_split(X_cluster, y_cluster, test_size=0.2, random_state=42)\n",
    "                    model = model_class(**params)\n",
    "                    model.fit(X_train, y_train)\n",
    "                    preds = model.predict(X_val)\n",
    "                    rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "                    return rmse\n",
    "                \n",
    "                study = optuna.create_study(direction=\"minimize\")\n",
    "                study.optimize(theobjective, n_trials=self.n_trials, timeout=self.timeout)\n",
    "\n",
    "                if study.best_value < best_rmse:\n",
    "                    best_rmse = study.best_value\n",
    "                    best_study = study\n",
    "                    best_model_type = name\n",
    "                    best_model = model_class(**study.best_params)\n",
    "                    best_model.fit(X_cluster, y_cluster)\n",
    "                \n",
    "                optuna_params[name] = study.best_params\n",
    "\n",
    "            if cluster_id in [1, 2]:\n",
    "                base_models = []\n",
    "                for base_name in [\"RandomForest\", \"GradientBoosting\", \"Ridge\"]:\n",
    "                    if base_name in self.model_types:\n",
    "                        model_class, param_space = self.model_types[base_name]\n",
    "                        params = optuna_params.get(base_name, {})\n",
    "                        base_model = model_class(**params)\n",
    "                        base_model.fit(X_cluster, y_cluster)\n",
    "                        base_models.append((base_name, base_model))\n",
    "\n",
    "                stacked_model = StackingRegressor(estimators=base_models, final_estimator=Ridge())\n",
    "                stacked_model.fit(X_cluster, y_cluster)\n",
    "                stacked_preds = stacked_model.predict(X_cluster)\n",
    "                stacked_rmse = np.sqrt(mean_squared_error(y_cluster, stacked_preds))\n",
    "\n",
    "                if stacked_rmse < best_rmse:\n",
    "                    best_rmse = stacked_rmse\n",
    "                    best_model_type = \"Stacking\"\n",
    "                    best_model = stacked_model\n",
    "                    best_study = None\n",
    "\n",
    "            self.cluster_models[cluster_id] = best_model\n",
    "            self.cluster_studies[cluster_id] = best_study\n",
    "            self.cluster_rmses[cluster_id] = best_rmse\n",
    "            self.cluster_model_types[cluster_id] = best_model_type\n",
    "            \n",
    "    def predicts(self, X, cluster_labels):\n",
    "        preds = np.zeros(len(X))\n",
    "        for cluster_id in range(self.n_clusters):\n",
    "            mask = cluster_labels == cluster_id\n",
    "            if np.sum(mask) == 0:\n",
    "                continue\n",
    "            X_cluster = X[mask]\n",
    "            model = self.cluster_models[cluster_id]\n",
    "            preds[mask] = model.predict(X_cluster)\n",
    "        return preds\n",
    "        \n",
    "    def report(self):\n",
    "        print(\"\\n--- Cluster Summary ---\")\n",
    "        for cluster_id in range(self.n_clusters):\n",
    "            print(f\"Cluster {cluster_id}: {self.cluster_model_types[cluster_id]} | RMSE: {self.cluster_rmses[cluster_id]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
